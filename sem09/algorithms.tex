%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% пакеты для математики
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}  
\mathtoolsset{showonlyrefs=true}  % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.

\usepackage[english, russian]{babel} % выбор языка для документа
% \usepackage[utf8]{inputenc}          % utf8 кодировка

% Основные шрифты 
\usepackage{fontspec}         
\setmainfont{Linux Libertine O}  % задаёт основной шрифт документа

% Математические шрифты 
\usepackage{unicode-math}     
\setmathfont[math-style=upright]{[Neo Euler.otf]} 


%%%%%%%%%% Работа с картинками и таблицами %%%%%%%%%%
\usepackage{graphicx} % Для вставки рисунков                
\usepackage{graphics}
\graphicspath{{images/}{pictures/}}   % папки с картинками

\usepackage{wrapfig}    % обтекание рисунков и таблиц текстом

\usepackage{booktabs}   % таблицы как в годных книгах
\usepackage{tabularx}   % новые типы колонок
\usepackage{tabulary}   % и ещё новые типы колонок
\usepackage{float}      % возможность позиционировать объекты в нужном месте
\renewcommand{\arraystretch}{1.2}  % больше расстояние между строками


%%%%%%%%%% Графики и рисование %%%%%%%%%%
\usepackage{tikz, pgfplots}  % языки для графики
\pgfplotsset{compat=1.16}

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos --- печатает все поставленные \todo'шки


%%%%%%%%%% Внешний вид страницы %%%%%%%%%%

\usepackage[paper=a4paper, top=20mm, bottom=15mm,left=20mm,right=15mm]{geometry}
\usepackage{indentfirst}    % установка отступа в первом абзаце главы

\usepackage{setspace}
\setstretch{1.15}  % межстрочный интервал
\setlength{\parskip}{4mm}   % Расстояние между абзацами
% Разные длины в LaTeX: https://en.wikibooks.org/wiki/LaTeX/Lengths

% свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее
\usepackage{microtype}

% \flushbottom                            % Эта команда заставляет LaTeX чуть растягивать строки, чтобы получить идеально прямоугольную страницу
\righthyphenmin=2                       % Разрешение переноса двух и более символов
\widowpenalty=300                     % Небольшое наказание за вдовствующую строку (одна строка абзаца на этой странице, остальное --- на следующей)
\clubpenalty=3000                     % Приличное наказание за сиротствующую строку (омерзительно висящая одинокая строка в начале страницы)
\tolerance=10000     % Ещё какое-то наказание.

% мои цвета https://www.artlebedev.ru/colors/
\definecolor{titleblue}{rgb}{0.2,0.4,0.6} 
\definecolor{blue}{rgb}{0.2,0.4,0.6} 
\definecolor{red}{rgb}{1,0,0.2} 
\definecolor{green}{rgb}{0,0.6,0} 
\definecolor{purp}{rgb}{0.4,0,0.8} 

% цвета из geogebra 
\definecolor{litebrown}{rgb}{0.6,0.2,0}
\definecolor{darkbrown}{rgb}{0.75,0.75,0.75}

% Гиперссылки
\usepackage{xcolor}   % разные цвета

\usepackage{hyperref}
\hypersetup{
	unicode=true,           % позволяет использовать юникодные символы
	colorlinks=true,       	% true - цветные ссылки
	urlcolor=blue,          % цвет ссылки на url
	linkcolor=red,          % внутренние ссылки
	citecolor=green,        % на библиографию
	breaklinks              % если ссылка не умещается в одну строку, разбивать её на две части?
}

% меняю оформление секций 
\usepackage{titlesec}
\usepackage{sectsty}

% меняю цвет на синий
\sectionfont{\color{titleblue}}
\subsectionfont{\color{titleblue}}

% выбрасываю нумерацию страниц и колонтитулы 
\pagestyle{empty}

% синие круглые бульпоинты в списках itemize 
\usepackage{enumitem}

\definecolor{itemizeblue}{rgb}{0, 0.45, 0.70}

\newcommand*{\MyPoint}{\tikz \draw [baseline, fill=itemizeblue, draw=blue] circle (2.5pt);}

\renewcommand{\labelitemi}{\MyPoint}

% расстояние в списках
\setlist[itemize]{parsep=0.4em,itemsep=0em,topsep=0ex}
\setlist[enumerate]{parsep=0.4em,itemsep=0em,topsep=0ex}


%%%%%%%%%% Свои команды %%%%%%%%%%

% Математические операторы первой необходимости:
\DeclareMathOperator{\sgn}{sign}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\E}{\mathop{E}}
\DeclareMathOperator{\Med}{Med}
\DeclareMathOperator{\Mod}{Mod}
\DeclareMathOperator*{\plim}{plim}

% команды пореже
\newcommand{\const}{\mathrm{const}}  % const прямым начертанием
\newcommand{\iid}{\sim i.\,i.\,d.}  % ну вы поняли...
\newcommand{\fr}[2]{\ensuremath{^{#1}/_{#2}}}   % особая дробь
\newcommand{\ind}[1]{\mathbbm{1}_{\{#1\}}} % Индикатор события
\newcommand{\dx}[1]{\,\mathrm{d}#1} % для интеграла: маленький отступ и прямая d

% одеваем шапки на частые штуки
\def \hb{\hat{\beta}}
\def \hs{\hat{s}}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \he{\hat{\varepsilon}}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}

% Греческие буквы
\def \a{\alpha}
\def \b{\beta}
\def \t{\tau}
\def \dt{\delta}
\def \e{\varepsilon}
\def \ga{\gamma}
\def \kp{\varkappa}
\def \la{\lambda}
\def \sg{\sigma}
\def \tt{\theta}
\def \Dt{\Delta}
\def \La{\Lambda}
\def \Sg{\Sigma}
\def \Tt{\Theta}
\def \Om{\Omega}
\def \om{\omega}

% Готика
\def \mA{\mathcal{A}}
\def \mB{\mathcal{B}}
\def \mC{\mathcal{C}}
\def \mE{\mathcal{E}}
\def \mF{\mathcal{F}}
\def \mH{\mathcal{H}}
\def \mL{\mathcal{L}}
\def \mN{\mathcal{N}}
\def \mU{\mathcal{U}}
\def \mV{\mathcal{V}}
\def \mW{\mathcal{W}}

% Жирные буквы
\def \mbb{\mathbb}
\def \RR{\mbb R}
\def \NN{\mbb N}
\def \ZZ{\mbb Z}
\def \PP{\mbb{P}}
\def \QQ{\mbb Q}


%%%%%%%%%% Теоремы %%%%%%%%%%
\theoremstyle{plain} % Это стиль по умолчанию.  Есть другие стили.
\newtheorem{theorem}{Теорема}[section]
\newtheorem{result}{Следствие}[theorem]
% счётчик подчиняется теоремному, нумерация идёт по главам согласованно между собой

% убирает курсив и что-то еще наверное делает ;)
\theoremstyle{definition}         
\newtheorem*{definition}{Определение}  % нумерация не идёт вообще


%%%%%%%%%% Задачки и решения %%%%%%%%%%
\usepackage{etoolbox}    % логические операторы для своих макросов
\usepackage{environ}
\newtoggle{lecture}

\newcounter{problem}%[section]  % счётчик для упражнений 

\renewcommand{\theproblem}{\arabic{problem}}

\newenvironment{problem}[1]{
\addtocounter{problem}{1}\noindent{ \color{titleblue} \large \bfseries Упражнение~\theproblem~#1 \vspace{1ex} \newline}
}{ }

% Окружение, чтобы можно было убирать решения из pdf
\NewEnviron{solution}{%
  \iftoggle{lecture}
    {\noindent \textbf{\large Решение:} \vspace{1ex} \newline \BODY}
    {}%
  }
  
% выделение по тексту важных вещей
\newcommand{\indef}[1]{\textbf{ \color{green} #1}} 


\begin{document} % Конец преамбулы, начало файла

% Если переключить в false, все solution исчезнут из pdf
\toggletrue{lecture}
%\togglefalse{lecture}
% эпиграфы
\usepackage{epigraph}
\setlength\epigraphwidth{.4\textwidth}
\setlength\epigraphrule{0pt}


\section*{Семинар 9: алгоритмы классификации}

\epigraph{Безумная цитата}{автор этой цитаты}

На прошлом семинаре мы говорили про то, какими бывают метрики классификации. Обычно их используют, чтобы сравнивать между собой различные алгоритмы для классификации. Ни одного такого алгоритма мы пока ещё не знаем. Пришло время исправить это досадное упущение. 

\begin{problem}{(KNN и кросс-валидация)}
На плоскости расположены колонии рыжих и чёрных муравьёв. Рыжих колоний три и они имеют координаты $(-1, -1)$, $(1, 1)$ и $(3, 3)$. Чёрных колоний тоже три и они имеют координаты $(2, 2)$, $(4, 4)$ и $(6, 6)$.

\begin{enumerate}
	\item[а)] Поделите плоскость на «зоны влияния» рыжих и чёрных муравьёв, используя метод одного ближайшего соседа.
	\item[б)] Поделите плоскость на «зоны влияния» рыжих и чёрных муравьёв, используя метод трёх ближайших соседей.
	\item[в)] С помощью кросс-валидации с выкидыванием отдельных наблюдений выберите оптимальное число соседей $k$ перебрав $k \in \{1, 3, 5\}$. Целевой функцией является количество верных предсказаний (accuracy).
\end{enumerate}
\end{problem}

\begin{solution} 
\begin{enumerate}
	\item[а)]  Будем ради удобства измерять расстояние между муравейниками в метрах. Давайте отметим на плоскости несколько рандомных точек и посмотрим к чьей зоне влияния они относятся. 
	
	\begin{center}
	    \includegraphics[scale=0.18]{2knn_1.png}
	\end{center} 
	
    Точка номер один явно будет в зоне влияния рыжих муравьёв. До ближайшего рыжего муравейника нужно пройти $\sqrt{5}$ метров, до ближайшего чёрного $3$ метра.  Точка два тоже рыжая. 
	
    По аналогии точки восемь и шесть оказываются чёрными. С оставшимися точками возникают проблемы. Например, от точки номер пять одинаковое расстояние как до чёрного, так и до рыжего муравейников. Она является спорной. Судя по всему, именно через неё пройдёт граница. Давайте попробуем нащупать побольше подобных пограничных точек. 
	
	Если точка принадлежит рыжим муравьям, будем помечать её рыжим крестом. Если чёрным, то чёрным. Если это спорная точка, то синим. 
	
	\begin{center}
	    \includegraphics[scale=0.18]{2knn_2.png}
	\end{center} 
	
	Кажется, что мы нащупали границы, вдоль которых находятся спорные территории. Осталось только прочертить их.	
	
	\begin{center}
	    \includegraphics[scale=0.18]{2knn_3.png}
	\end{center} 
	
	\item[б)]  Теперь попробуем поделить плоскость на зоны влияния, используя метод трёх ближайших соседей. Посмотрим на самую первую картинку, где мы нанесли на плоскость рандомные точки, и попробуем порассуждать в чьей зоне влияния оказывается какая точка. 
	
	Для первой точки две из трёх ближайших --- рыжие. Она находится в рыжей зоне влияния. По аналогии происходит со второй и третьей точками.  Пятая, шестая, седьмая и восьмая точки оказываются в зоне влияния чёрных муравьёв и окрашиваются в чёрные цвета. Проблемы возникают только с четвёртой точкой. Ближайшие к ней две точки --- рыжая и чёрная. Решение надо принимать по третьему ближайшему соседу. Третью ближайшую точку найти не удаётся, так как рыжая и чёрная точка находятся от неё н одинаковых расстояниях. Выходит, что мы оказались на границе.
	
	\begin{center}
	    \includegraphics[scale=0.18]{2knn_4.png}
	\end{center} 

	Попробуем нащупать ещё пограничных точек и провести пограничную линию. 

	\begin{center}
	    \includegraphics[scale=0.18]{2knn_5.png}
	\end{center} 	

	И это граница? У нас же есть ошибки!  Да, есть. Давайте вспомним упражнение про собачек, кошек, пиццу и бургеры с прошлого семинара. Когда мы решали его, мы поняли, что слишком детализированная граница между классами приводит к переобучению. 

	Порассуждаем в терминах джунглей. Есть поле, на нём селятся муравьи. Логично ли с их стороны селиться полосками? Конечно же нет. Намного логичнее было бы, что по историческим причинам на одной стороне поля живут рыжие муравьи, на второй чёрные. У нас в выборке оказалось несколько примеров муравейников. И по ним мы попытались нащупать границу для зон влияния. На границе вполне может происходить такое, что муравьи проникают на территорию друг-друга. 

	Проводя излишние полосы, мы переходим от выуживания реальных закономерностей, существующих в джунглях, к излишнему фрагментированию обучающей выборки, то есть переобучаемся под её особенности. Практически все алгоритмы машинного обучения страдают этим грехом. Нам надо будет как следует бить их за переобучение и исправлять.

	\item[в)]  Давайте убедимся в том, что алгоритм трёх ближайших соседей, проводящий одну разграничительную линию между муравьями, работает лучше, чем алгоритм одного ближайшего соседа. Для этого воспользуемся стратегией кросс-валидации. 

	Кросс-валидация состоит в следующем: давайте будем закрывать по очереди разные части выборки ладошкой. На оставшейся выборке будем обучать модель, а на скрытой проверять её качество. Будем делать так много раз и посмотрим на итоговое качество. 
	
	\begin{minipage}[t]{0.45\textwidth}
	    \includegraphics[scale=0.15]{2knn_6.png}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\textwidth}
	    \includegraphics[scale=0.15]{2knn_7.png}
	\end{minipage}
	
	Закрываем ладошкой самую нижнюю точку. По оставшимся четырём расчерчиваем границы. Мы по методу одного ближайшего соседа относим эту точку к рыжим муравьям. Это оказывается правильным решением. Угадали.

	Закроем ладошкой вторую снизу точку. Расчертим границы. Она окажется ближе всего к чёрным муравьям. Но на самом деле она рыжая. Ошибка... Также проделаем с остальными точками. В итоге получится, что мы совершаем целых $4$ ошибки. По аналогии сделаем с методом трёх ближайших соседей и получим всего лишь $2$ ошибки. 

	Чувствуете? Мы ошибаемся из-за излишней детализации, которую нам навязывает метод одного ближайшего соседа. Кросс-валидация позволяет это отследить. А что, если взять $5$ ближайших соседей? Тогда мы ошибёмся абсолютно в каждой точке. Попробуйте проделать это. 

	На самом деле $k$ это \indef{гиперпараметр} метода ближайших соседей. Мы можем подобрать его оптимальным образом с помощью кросс-валидации. В данном примере оптимально будет выбрать $k=3$. 
	
	В этом упражнении мы использовали для оптимизации алгоритма метрику accuracy. Понятное дело, что можно пытаться использовать и любую другую метрику с прошлого семинара. 
	\end{enumerate}
\end{solution}

\begin{problem}{(древо для классификации)}
У Маши есть инстаграмчик. На неё подписана целая куча парней. Недавно Дима поставил ей $10$ лайков. Тогда Маша схватила ромашку и начала гадать. Она нагадала, что Дима плюнет в неё. То же самое она сделала с другими парнями. 

Результат гадания --- переменная $y_i$, количество лайков у фотки --- переменная $x_i$. 

\begin{center}
	\begin{tabular}{cc}
		$y_i$ & $x_i$ \\
		\hline
		плюнет & $10$ \\
		поцелует & $11$ \\
		поцелует & $12$ \\
		к сердцу прижмёт & $13$ \\
		к сердцу прижмёт & $14$ \\
	\end{tabular}
\end{center}

Сегодня в Машином инстаграмме Джонни Деп поставил $15$ лайков. Маше очень хочется понадать что же с ней сделает Джонни Деп, но у неё кончились ромашки. Поэтому она решила обучить классификационное дерево, которое поможет ей спрогнозировать $y_i$ по $x_i$.

Дерево строится до идеальной классификации. Критерий деления узла на два --- минимизация числа допущенных ошибок\footnote{На самом деле на практике так не делают. Обычно для разбиения узла при строительстве классификационных деревьев используют энтропию. О том, что это такое, можно погуглить.}.  Правило прогнозирования в каждой вершине: в качестве прогноза выдаем тот класс, представителей которого в вершине больше.
\end{problem}

\begin{solution}
Мы должны обучить дерево, которое будет по переменной $x$, число лайков от парня, прогнозировать переменную $y$, состояние отношений Маши.  Обычно деревья учат по-жадному. Будем смотреть, какое разбиение по переменной $x$ сильнее всего уменьшает ошибку, и выбирать его. 
	
Ошибку мы договорились считать как долю неверных ответов. Обычно на практике при разбиении вершины на две используют не такой критерий, но мы для простоты используем его.

Будем перебирать все возможные пороги и смотреть что получится.
	
При делении вершины на две между $10$ и $11$ лайками, слева у нас окажется плюнет. Именно его мы и будем там прогнозировать. Справа окажется два поцелует и два к сердцу прижмёт. Надо спрогнозировать в этой вершине класс, представителей которого тут большинство, чтобы сделать поменьше ошибок. Так как у нас оба класса представлены в одинаковом объёме, неважно что мы спрогнозируем. В любом случае получим две ошибки. 
	
При дроблении вершины на две между $11$ и $12$ лайками, слева оказывается плюнет и поцелует. Одна ошибка. Справа оказывается два к сердцу прижмёт и одно поцелует. Спрогнозируем к сердцу прижмёт, так как их большинство, и получи одну ошибку. В сумме у нас две ошибки. 
	
\begin{center}
    \includegraphics[scale=0.28]{class_tree_1.png}
\end{center} 	
	
Рассуждая аналогичном образом приходим к выводу, что самое классное разбиение между $12$ и $13$. При нём мы совершаем только одну ошибку.  В дереве, мы будем задавать вопрос: <<А количество лайков меньше $13$?>> Если да, будем идти налево и прогнозировать, что нас поцелуют. Если нет, будем идти направо и прогнозировать, что нас прижмут к сердцу. 
	
Справа в листе дерева у нас оказались объекты одного класса. Слева в листе дерева соодержутся объекты разных классов. Можно сделать ещё одно разбиение. 
	
\begin{center}
	\includegraphics[scale=0.28]{class_tree_2.png}
\end{center} 	

В итоге в нашем дереве окажется три листа, в каждом из которых мы будем делать прогноз. Обратите внимание, что дерево запомнило выборку.  Деревья постоянно так делают. В этом их сушественный минус. Чтобы победить его, деревья стригут. Либо используют как части более сложных моделей. Например, как часть случайного леса. 
	
Джонни поставил Маше $15$ лайков. Что же ожидает их отношения? Начинаем идти по решающему дереву, чтобы сделать прогноз. Число лайков меньше $13$? Нет. Идём направо. Кажется, Машу прижмут к сердцу. Это наш прогноз.  	
\end{solution}

\begin{problem}{(ещё деревья)}
Ниже изображены разделяющие поверхности для задачи бинарной классификации, соответствующие решающим деревьям разной глубины. Какое из изображений соответствует наиболее глубокому дереву? Какой примерной глубине дерева соответствует каждая из картинок? 

\begin{center}
	\includegraphics[scale=0.6]{trees.png}
\end{center}
\end{problem}

\begin{solution}
Чем глубже дерево, тем сильнее оно фрагментирует нашу выборку, и тем сильнее оно выделяет в ней самые микроскопические кусочки.  Сильнее всего выборка фрагментирована на верхней правой картинке, значит это разбиение плоскости на части соответствует самому глубокому дереву. 

На третьей картинке плоскость дробиться на части один раз. Значит в дереве есть один сплит. Его глубина равна единице. На первой картинке появляется ещё одно дополнительное разбиение по оси $y$, глубина дерева увеличивается до двух. 

На картинке номер $4$ мы делаем два дополнительных разбиения правой части и два левой. Глубина дерева уже не менее трёх. На второй картинке всё становится ещё глубже. 
\end{solution}

\section*{Ещё задачи} 

Тут лежит ещё несколько задач для самостоятельного решения. Возможно, похожие будут в самостоятельной работе... 

\begin{problem}{}
На плоскости расположены колонии рыжих и чёрных муравьёв. Рыжих колоний три и они имеют координаты $(-1, 1)$, $(1, -1)$ и $(1, 1)$. Чёрных колоний одна и она имеет координаты $(0, 0)$.

\begin{enumerate}
\item[а)] Поделите плоскость на «зоны влияния» рыжих и чёрных используя метод одного и трёх ближайших соседей.

\item[б)] С помощью кросс-валидации с выкидыванием отдельных наблюдений выберите оптимальное число соседей $k$ перебрав $k \in \{1, 3 \}$. Целевой функцией является количество несовпадающих прогнозов.
\end{enumerate}
\end{problem}

\begin{problem}{}
Объясните мемас: 

\begin{center}
	\includegraphics[scale=0.3]{memes2.jpg}
\end{center}
\end{problem}

\begin{problem}{}
Пятачок собрал данные о визитах Винни-Пуха в гости к Кролику. Здесь $x_i$ --- количество съеденного мёда в горшках, а $y_i$  --- бинарная переменная, отражающая застревание Винни-Пуха при входе 

\begin{center}
	\begin{tabular}{c|c}
		$y_i$ & $x_i$ \\
		\hline
		0  & 1 \\
		1 & 4\\
		1 & 2\\
		0 & 3 \\
		1 & 3 \\
		0 & 1
	\end{tabular}
\end{center}

\begin{enumerate}
	\item[а)] Пятачок собирается оценить дерево по всей выборке.  Помогите очень маленькому существу сделать это. 
	\item[б)] Пятачок узнал у Иа-Иа, что оказывается выборку надо делить на тренировочную и тестовую Поэтому он отложил последние два наблюдения для теста. Оцените дерево по первым четырём наблюдениям и проверьте его работоспособность по последним двум. 
	\item[в)]  Пятачок поговорил с Совой и узнал, что деревья часто переобучаются. Она рассказала ему, что над деревьями надо строить ансамбли. Например, случайный лес. Пятачок решил построить лес из трёх деревьев. Первое дерево он строит на наблюдениях с первого по третье, второе на наблюдениях со второго по четвёртое.  Третье дерево на наблюдениях $1,2,4$. Помогите пяточку построить лес и оценить качество его работы на тестовой выборке. 
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
	\item[а)]  Бедный малютка Пяточок! Он даже не понимал, на какие муки он себя обрекает, когда собирался строить свою модель для прогнозирования того, что произойдёт с Винни! Как же хорошо, что мы оказались рядом и подставили маленькому существу своё большое дружеское плечо.  Для начала построим дерево сразу на всей выборке. 
	
	\begin{center}
	    \includegraphics[scale=0.28]{class_tree_6.png}
	\end{center} 	

	Обратите внимание, что это дерево ошибается из-за того, что при  $x=3$ у нас есть как факт застревания медведя в норе, так и факт его прохождения сквозь нору. Когда мы оказываемся в вершине, где одинаковое число нулей и единиц, нам придется принимать решение случайно. Компьютер поступает именно так. На больших выборках это случаентся довольно редко. 

	\item[б)]  Когда мы строим дерево на первых четырёх наблюдениях, первое разбиение можно сделать либо по единице, либо по четвёрке. В обеих ситуациях совершается одна ошибка. Для удобства выберем первый случай. Дальше снова неважно где делать разбиение. Сделаем его в двойке. В итоге получим дерево из пункта а). На тестовой выборке дерево делает одну ошибку при $x=3$.
	
	\item[в)]   Лес, который должен получиться в ходе обучения, изображён на кратинке: 
	
	
	\begin{center}
		\includegraphics[scale=0.17]{forest.png}
	\end{center} 	
	
	Для $x=3$ первое дерево прогнозирует $1$, второе $0$, третье $0$. Большая часть леса говорит, что прогнозом будет $0$. Верим в торжество демократии и берём его в качестве итогового прогноза. Допускаем ошибку\footnote{Демократия не идеальна, но это лучшее, что придумало человечество}. Для $x=1$ первое и третье деревья прогнозируют $0$, второе $1$, берём ноль и не ошибаемся. Прогноз по лесу построен.
	
	На практике случайный лес показывает себя как очень мощный алгоритм. Не стесняйтесь использовать его. 
\end{enumerate}
\end{solution}

\begin{problem}{}
По данной диаграмме рассеяния постройте классификационное дерево для зависимой переменной $y$:

\begin{center}
	\begin{tikzpicture}[scale = 0.015]
	\input{images/tree_scatter_data.tikz}
	\end{tikzpicture}
\end{center}
\end{problem}

\begin{solution}
\begin{center}
	\includegraphics[scale=0.2]{class_tree_3.png}
\end{center} 	
	
Если мы дробим сначала по оси $y$, то мы сразу же довольно сильно уменьшаем неопределённость и ошибаемся только на верхнем левом прямоугольнике, прогнозируя, что он синего цвета. 
	
Если мы дробим сначала по переменной $x$, то мы будем ошибаться на нижнем правом прямоугольнике. Там ошибка намного страшнее. Значит, сначала произойдёт разбиение по $y$, затем по $x$. Именно в этом состоит жадная процедура обучения дерева: уменьшить ошибку при каждом разбиении как можно сильнее. 
\end{solution}

\begin{problem}{}
Рассмотрим обучающую выборку для прогнозирования $y$ с помощью $x$ и $z$:

\begin{center}
	\begin{tabular}{c|c|c}
		$y_i$ & $x_i$ & $z_i$ \\
		\hline
		$y_1$ & $1$ & $2$ \\
		$y_2$ & $1$ & $2$ \\
		$y_3$ & $2$ & $2$ \\
		$y_4$ & $2$ & $1$\\
		$y_5$ & $2$ & $1$ \\
		$y_6$ & $2$ & $1$ \\
		$y_7$ & $2$ & $1$ \\
	\end{tabular}
\end{center}

Будем называть деревья разными, если они выдают разные прогнозы на обучающей выборке. Сколько существует разных классификационных деревьев  для данного набора данных?
\end{problem}

\begin{solution}
Либо мы сначала дробим по $x$, потом по $z$. Либо наоборот. 	\end{solution}

\end{document}